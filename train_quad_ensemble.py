"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è QuadEnsemble –º–æ–¥–µ–ª–∏ (RF + XGBoost + LightGBM + LSTM).
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python train_quad_ensemble.py --symbol BTCUSDT --days 180
"""
import warnings
import os
import argparse
import sys
from pathlib import Path

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
os.environ['PYTHONWARNINGS'] = 'ignore::UserWarning'
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent))

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("‚ùå ERROR: PyTorch is not installed!")
    print("   Install with: pip install torch>=2.0.0")

from bot.config import load_settings
from bot.ml.data_collector import DataCollector
from bot.ml.feature_engineering import FeatureEngineer
from bot.ml.model_trainer import ModelTrainer, LIGHTGBM_AVAILABLE, LSTM_AVAILABLE


def main():
    parser = argparse.ArgumentParser(description='Train QuadEnsemble ML model (RF+XGB+LGB+LSTM)')
    parser.add_argument('--symbol', type=str, default='BTCUSDT', 
                       help='Trading symbol (default: BTCUSDT)')
    parser.add_argument('--days', type=int, default=180,
                       help='Number of days of historical data (default: 180)')
    parser.add_argument('--interval', type=str, default='15m',
                       help='Timeframe interval (default: 15m)')
    
    # RandomForest –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--rf_n_estimators', type=int, default=100,
                       help='Number of RF estimators (default: 100)')
    parser.add_argument('--rf_max_depth', type=int, default=10,
                       help='RF max depth (default: 10)')
    
    # XGBoost –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--xgb_n_estimators', type=int, default=100,
                       help='Number of XGB estimators (default: 100)')
    parser.add_argument('--xgb_max_depth', type=int, default=6,
                       help='XGB max depth (default: 6)')
    parser.add_argument('--xgb_learning_rate', type=float, default=0.1,
                       help='XGB learning rate (default: 0.1)')
    
    # LightGBM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--lgb_n_estimators', type=int, default=100,
                       help='Number of LGB estimators (default: 100)')
    parser.add_argument('--lgb_max_depth', type=int, default=6,
                       help='LGB max depth (default: 6)')
    parser.add_argument('--lgb_learning_rate', type=float, default=0.1,
                       help='LGB learning rate (default: 0.1)')
    
    # LSTM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--lstm_sequence_length', type=int, default=60,
                       help='LSTM sequence length (default: 60)')
    parser.add_argument('--lstm_hidden_size', type=int, default=64,
                       help='LSTM hidden size (default: 64)')
    parser.add_argument('--lstm_num_layers', type=int, default=2,
                       help='LSTM number of layers (default: 2)')
    parser.add_argument('--lstm_epochs', type=int, default=50,
                       help='LSTM training epochs (default: 50)')
    parser.add_argument('--lstm_batch_size', type=int, default=32,
                       help='LSTM batch size (default: 32)')
    parser.add_argument('--lstm_learning_rate', type=float, default=0.001,
                       help='LSTM learning rate (default: 0.001)')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    if not TORCH_AVAILABLE:
        return
    
    if not LIGHTGBM_AVAILABLE:
        print("‚ùå ERROR: LightGBM is not installed!")
        print("   Install with: pip install lightgbm>=4.0.0")
        return
    
    if not LSTM_AVAILABLE:
        print("‚ùå ERROR: LSTM module is not available!")
        print("   Check that bot.ml.lstm_model can be imported")
        return
    
    print("=" * 80)
    print("üöÄ QuadEnsemble ML Model Training (RF + XGBoost + LightGBM + LSTM)")
    print("=" * 80)
    print(f"Symbol: {args.symbol}")
    print(f"Days: {args.days}")
    print(f"Interval: {args.interval}")
    print(f"\nüìä Model Parameters:")
    print(f"  RandomForest: {args.rf_n_estimators} trees, max_depth={args.rf_max_depth}")
    print(f"  XGBoost: {args.xgb_n_estimators} trees, max_depth={args.xgb_max_depth}, lr={args.xgb_learning_rate}")
    print(f"  LightGBM: {args.lgb_n_estimators} trees, max_depth={args.lgb_max_depth}, lr={args.lgb_learning_rate}")
    print(f"  LSTM: seq_len={args.lstm_sequence_length}, hidden={args.lstm_hidden_size}, layers={args.lstm_num_layers}, epochs={args.lstm_epochs}")
    print("=" * 80)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    settings = load_settings()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ MTF-—Ä–µ–∂–∏–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ (—á–∏—Ç–∞–µ–º –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è)
    ml_mtf_enabled_env = os.getenv("ML_MTF_ENABLED", "1")
    ml_mtf_enabled = ml_mtf_enabled_env not in ("0", "false", "False", "no")
    mode_suffix = "mtf" if ml_mtf_enabled else "15m"
    
    # === –®–∞–≥ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ===
    if ml_mtf_enabled:
        print(f"\n[Step 1] Collecting historical data (15m, 1h, 4h) for {args.symbol}...")
    else:
        print(f"\n[Step 1] Collecting historical data (15m only) for {args.symbol}...")
    collector = DataCollector(settings.api)
    
    # –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–æ–∂–∏–¥–∞–µ–º '15m')
    base_interval = args.interval.replace('m', '')
    
    if ml_mtf_enabled:
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å—Ä–∞–∑—É –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        mtf_data = collector.collect_multiple_timeframes(
            symbol=args.symbol,
            intervals=[base_interval, "60", "240"],  # 15m, 1h, 4h
            start_date=None,
            end_date=None,
        )
        
        df_raw_15m = mtf_data.get(base_interval)
        df_raw_1h = mtf_data.get("60")
        df_raw_4h = mtf_data.get("240")
        
        if df_raw_15m is None or df_raw_15m.empty:
            print(f"‚ùå No 15m data collected for {args.symbol}. Skipping.")
            return
        
        print(f"‚úÖ Collected {len(df_raw_15m)} candles on 15m timeframe")
    else:
        # –°—Ç–∞—Ä—ã–π —Ä–µ–∂–∏–º: —Å–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ 15m –¥–∞–Ω–Ω—ã–µ
        df_raw_15m = collector.collect_klines(
            symbol=args.symbol,
            interval=base_interval,
            start_date=None,
            end_date=None,
            limit=200,
        )
        if df_raw_15m.empty:
            print(f"‚ùå No 15m data collected for {args.symbol}. Skipping.")
            return
        print(f"‚úÖ Collected {len(df_raw_15m)} candles on 15m timeframe (no higher TF)")
    
    # === –®–∞–≥ 2: Feature Engineering (–≤–∫–ª—é—á–∞—è MTF –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏) ===
    print(f"\n[Step 2] Creating features{' (including higher timeframes)' if ml_mtf_enabled else ' (15m only)'}...")
    feature_engineer = FeatureEngineer()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ –±–∞–∑–æ–≤–æ–º –¢–§ (15m)
    df_features = feature_engineer.create_technical_indicators(df_raw_15m)
    
    if ml_mtf_enabled:
        # –î–æ–±–∞–≤–ª—è–µ–º –º—É–ª—å—Ç–∏‚Äë—Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (1h, 4h), –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å
        higher_timeframes = {}
        df_raw_1h = mtf_data.get("60")
        df_raw_4h = mtf_data.get("240")
        if df_raw_1h is not None and not df_raw_1h.empty:
            higher_timeframes["60"] = df_raw_1h
        if df_raw_4h is not None and not df_raw_4h.empty:
            higher_timeframes["240"] = df_raw_4h
        
        if higher_timeframes:
            df_features = feature_engineer.add_mtf_features(df_features, higher_timeframes)
            print(f"‚úÖ Created {len(feature_engineer.get_feature_names())} features (with MTF)")
        else:
            print("‚ö†Ô∏è Could not collect 1h/4h data ‚Äî training on 15m features only.")
            print(f"‚úÖ Created {len(feature_engineer.get_feature_names())} features")
    else:
        print(f"‚úÖ Created {len(feature_engineer.get_feature_names())} features (15m only)")
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    print(f"\n[Step 3] Creating target variable...")
    df_with_target = feature_engineer.create_target_variable(
        df_features,
        forward_periods=5,  # 5 * 15m = 75 –º–∏–Ω—É—Ç
        threshold_pct=1.0,  # 1.0% –ø–æ—Ä–æ–≥
        use_atr_threshold=True,
        use_risk_adjusted=True,
        min_risk_reward_ratio=2.0,  # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ä–∏—Å–∫/–ø—Ä–∏–±—ã–ª—å 2:1 (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º TP=25%, SL=10%)
        max_hold_periods=48,  # –ú–∞–∫—Å–∏–º—É–º 48 * 15m = 12 —á–∞—Å–æ–≤ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–¥–µ–ª–æ–∫ (—Å–º—è–≥—á–µ–Ω–æ: –±—ã–ª–æ 32)
        min_profit_pct=1.0,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å 1.0% –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–∞–∫ LONG/SHORT (—Å–º—è–≥—á–µ–Ω–æ: –±—ã–ª–æ 1.5%)
    )
    
    target_dist = df_with_target['target'].value_counts().to_dict()
    print(f"‚úÖ Target distribution:")
    for target_val, count in sorted(target_dist.items()):
        pct = (count / len(df_with_target)) * 100
        target_name = {-1: "SHORT", 0: "HOLD", 1: "LONG"}.get(
            target_val, f"UNKNOWN({target_val})")
        print(f"    {target_name:6s}: {count:5d} ({pct:5.1f}%)")
    
    # === –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML ===
    print(f"\n[Step 4] Preparing data for ML...")
    X, y = feature_engineer.prepare_features_for_ml(df_with_target)
    print(f"‚úÖ Prepared data: X.shape={X.shape}, y.shape={y.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è LSTM
    if len(df_with_target) < args.lstm_sequence_length + 100:
        print(f"‚ö†Ô∏è  WARNING: Not enough data for LSTM (need at least {args.lstm_sequence_length + 100} rows, got {len(df_with_target)})")
        print(f"   Consider reducing --lstm_sequence_length or collecting more data")
    
    # === –®–∞–≥ 5: –û–±—É—á–µ–Ω–∏–µ QuadEnsemble ===
    print(f"\n[Step 5] Training QuadEnsemble...")
    print(f"   This will train 4 models sequentially:")
    print(f"   1. RandomForest")
    print(f"   2. XGBoost")
    print(f"   3. LightGBM")
    print(f"   4. LSTM (this may take longer)")
    print()
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Ñ–æ–∫—É—Å–∞ –Ω–∞ –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —Å–¥–µ–ª–∫–∞—Ö
    from sklearn.utils.class_weight import compute_class_weight
    import numpy as np
    
    classes = np.unique(y)
    base_weights = compute_class_weight('balanced', classes=classes, y=y)
    
    # –£–°–ò–õ–ï–ù–ù–´–ï –≤–µ—Å–∞ –¥–ª—è LONG/SHORT, –ú–ò–ù–ò–ú–ò–ó–ò–†–£–ï–ú HOLD
    class_weight_dict = {}
    for i, cls in enumerate(classes):
        if cls == 0:  # HOLD
            class_weight_dict[cls] = base_weights[i] * 0.1  # –°–∏–ª—å–Ω–æ —É–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å HOLD
        else:  # LONG or SHORT
            class_weight_dict[cls] = base_weights[i] * 3.0  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å LONG/SHORT
    
    print(f"   üìä Class weights:")
    for cls, weight in class_weight_dict.items():
        label_name = "LONG" if cls == 1 else ("SHORT" if cls == -1 else "HOLD")
        print(f"      {label_name}: {weight:.3f}")
    print()
    
    trainer = ModelTrainer()
    
    try:
        model, metrics = trainer.train_quad_ensemble(
            X=X,
            y=y,
            df=df_with_target,  # –ü–æ–ª–Ω—ã–π DataFrame –¥–ª—è LSTM
            rf_n_estimators=args.rf_n_estimators,
            rf_max_depth=args.rf_max_depth,
            xgb_n_estimators=args.xgb_n_estimators,
            xgb_max_depth=args.xgb_max_depth,
            xgb_learning_rate=args.xgb_learning_rate,
            lgb_n_estimators=args.lgb_n_estimators,
            lgb_max_depth=args.lgb_max_depth,
            lgb_learning_rate=args.lgb_learning_rate,
            lstm_sequence_length=args.lstm_sequence_length,
            lstm_hidden_size=args.lstm_hidden_size,
            lstm_num_layers=args.lstm_num_layers,
            lstm_epochs=args.lstm_epochs,
            class_weight=class_weight_dict,  # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        )
        
        print(f"\nüìä QuadEnsemble Results:")
        print(f"  RandomForest CV Accuracy: {metrics['rf_metrics']['cv_mean']:.4f} (+/- {metrics['rf_metrics']['cv_std'] * 2:.4f})")
        print(f"  XGBoost CV Accuracy: {metrics['xgb_metrics']['cv_mean']:.4f} (+/- {metrics['xgb_metrics']['cv_std'] * 2:.4f})")
        print(f"  LightGBM CV Accuracy: {metrics['lgb_metrics']['cv_mean']:.4f} (+/- {metrics['lgb_metrics']['cv_std'] * 2:.4f})")
        print(f"  LSTM Accuracy: {metrics['lstm_metrics'].get('accuracy', 0):.4f}")
        print(f"\n  Ensemble Weights:")
        print(f"    RF:   {metrics['rf_weight']:.3f}")
        print(f"    XGB:  {metrics['xgb_weight']:.3f}")
        print(f"    LGB:  {metrics['lgb_weight']:.3f}")
        print(f"    LSTM: {metrics['lstm_weight']:.3f}")
        
    except Exception as e:
        print(f"‚ùå ERROR during training: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # === –®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    print(f"\n[Step 6] Saving model...")
    model_filename = f"quad_ensemble_{args.symbol}_{args.interval.replace('m', '')}_{mode_suffix}.pkl"
    
    try:
        trainer.save_model(
            model,
            trainer.scaler,
            feature_engineer.get_feature_names(),
            metrics,
            model_filename,
            symbol=args.symbol,
            interval=args.interval.replace('m', ''),
            model_type="quad_ensemble",
        )
        
        print(f"‚úÖ Model saved: {model_filename}")
        print(f"\nüéâ Training completed successfully!")
        print(f"\nüí° Next steps:")
        print(f"   1. Test the model: python -m bot.ml.diagnose_model ml_models/{model_filename}")
        print(f"   2. Backtest: python backtest_ml_strategy.py --model ml_models/{model_filename} --symbol {args.symbol} --days 30")
        print(f"   3. Use in live trading: Enable ML strategy in config")
        print(f"   4. Compare with other models: Check backtest results")
        
    except Exception as e:
        print(f"‚ùå ERROR saving model: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
