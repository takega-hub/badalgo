"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è QuadEnsemble –º–æ–¥–µ–ª–∏ (RF + XGBoost + LightGBM + LSTM).
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python train_quad_ensemble.py --symbol BTCUSDT --days 180
"""
import warnings
import os
import argparse
import sys
from pathlib import Path

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
os.environ['PYTHONWARNINGS'] = 'ignore::UserWarning'
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent))

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("‚ùå ERROR: PyTorch is not installed!")
    print("   Install with: pip install torch>=2.0.0")

from bot.config import load_settings
from bot.ml.data_collector import DataCollector
from bot.ml.feature_engineering import FeatureEngineer
from bot.ml.model_trainer import ModelTrainer, LIGHTGBM_AVAILABLE, LSTM_AVAILABLE


def main():
    parser = argparse.ArgumentParser(description='Train QuadEnsemble ML model (RF+XGB+LGB+LSTM)')
    parser.add_argument('--symbol', type=str, default='BTCUSDT', 
                       help='Trading symbol (default: BTCUSDT)')
    parser.add_argument('--days', type=int, default=180,
                       help='Number of days of historical data (default: 180)')
    parser.add_argument('--interval', type=str, default='15m',
                       help='Timeframe interval (default: 15m)')
    
    # RandomForest –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--rf_n_estimators', type=int, default=100,
                       help='Number of RF estimators (default: 100)')
    parser.add_argument('--rf_max_depth', type=int, default=10,
                       help='RF max depth (default: 10)')
    
    # XGBoost –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--xgb_n_estimators', type=int, default=100,
                       help='Number of XGB estimators (default: 100)')
    parser.add_argument('--xgb_max_depth', type=int, default=6,
                       help='XGB max depth (default: 6)')
    parser.add_argument('--xgb_learning_rate', type=float, default=0.1,
                       help='XGB learning rate (default: 0.1)')
    
    # LightGBM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--lgb_n_estimators', type=int, default=100,
                       help='Number of LGB estimators (default: 100)')
    parser.add_argument('--lgb_max_depth', type=int, default=6,
                       help='LGB max depth (default: 6)')
    parser.add_argument('--lgb_learning_rate', type=float, default=0.1,
                       help='LGB learning rate (default: 0.1)')
    
    # LSTM –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument('--lstm_sequence_length', type=int, default=60,
                       help='LSTM sequence length (default: 60)')
    parser.add_argument('--lstm_hidden_size', type=int, default=64,
                       help='LSTM hidden size (default: 64)')
    parser.add_argument('--lstm_num_layers', type=int, default=2,
                       help='LSTM number of layers (default: 2)')
    parser.add_argument('--lstm_epochs', type=int, default=50,
                       help='LSTM training epochs (default: 50)')
    parser.add_argument('--lstm_batch_size', type=int, default=32,
                       help='LSTM batch size (default: 32)')
    parser.add_argument('--lstm_learning_rate', type=float, default=0.001,
                       help='LSTM learning rate (default: 0.001)')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    if not TORCH_AVAILABLE:
        return
    
    if not LIGHTGBM_AVAILABLE:
        print("‚ùå ERROR: LightGBM is not installed!")
        print("   Install with: pip install lightgbm>=4.0.0")
        return
    
    if not LSTM_AVAILABLE:
        print("‚ùå ERROR: LSTM module is not available!")
        print("   Check that bot.ml.lstm_model can be imported")
        return
    
    print("=" * 80)
    print("üöÄ QuadEnsemble ML Model Training (RF + XGBoost + LightGBM + LSTM)")
    print("=" * 80)
    print(f"Symbol: {args.symbol}")
    print(f"Days: {args.days}")
    print(f"Interval: {args.interval}")
    print(f"\nüìä Model Parameters:")
    print(f"  RandomForest: {args.rf_n_estimators} trees, max_depth={args.rf_max_depth}")
    print(f"  XGBoost: {args.xgb_n_estimators} trees, max_depth={args.xgb_max_depth}, lr={args.xgb_learning_rate}")
    print(f"  LightGBM: {args.lgb_n_estimators} trees, max_depth={args.lgb_max_depth}, lr={args.lgb_learning_rate}")
    print(f"  LSTM: seq_len={args.lstm_sequence_length}, hidden={args.lstm_hidden_size}, layers={args.lstm_num_layers}, epochs={args.lstm_epochs}")
    print("=" * 80)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    settings = load_settings()
    
    # === –®–∞–≥ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ===
    print(f"\n[Step 1] Collecting historical data for {args.symbol}...")
    collector = DataCollector(settings.api)
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df_raw = collector.collect_klines(
        symbol=args.symbol,
        interval=args.interval.replace('m', ''),
        start_date=None,
        end_date=None,
        limit=200,
    )
    
    if df_raw.empty:
        print(f"‚ùå No data collected for {args.symbol}. Skipping.")
        return
    
    print(f"‚úÖ Collected {len(df_raw)} candles")
    
    # === –®–∞–≥ 2: Feature Engineering ===
    print(f"\n[Step 2] Creating features...")
    feature_engineer = FeatureEngineer()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    df_features = feature_engineer.create_technical_indicators(df_raw)
    print(f"‚úÖ Created {len(feature_engineer.get_feature_names())} features")
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    print(f"\n[Step 3] Creating target variable...")
    df_with_target = feature_engineer.create_target_variable(
        df_features,
        forward_periods=5,  # 5 * 15m = 75 –º–∏–Ω—É—Ç
        threshold_pct=1.0,  # 1.0% –ø–æ—Ä–æ–≥
        use_atr_threshold=True,
        use_risk_adjusted=True,
        min_risk_reward_ratio=1.5,
    )
    
    target_dist = df_with_target['target'].value_counts().to_dict()
    print(f"‚úÖ Target distribution:")
    for target_val, count in sorted(target_dist.items()):
        pct = (count / len(df_with_target)) * 100
        target_name = {-1: "SHORT", 0: "HOLD", 1: "LONG"}.get(
            target_val, f"UNKNOWN({target_val})")
        print(f"    {target_name:6s}: {count:5d} ({pct:5.1f}%)")
    
    # === –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML ===
    print(f"\n[Step 4] Preparing data for ML...")
    X, y = feature_engineer.prepare_features_for_ml(df_with_target)
    print(f"‚úÖ Prepared data: X.shape={X.shape}, y.shape={y.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è LSTM
    if len(df_with_target) < args.lstm_sequence_length + 100:
        print(f"‚ö†Ô∏è  WARNING: Not enough data for LSTM (need at least {args.lstm_sequence_length + 100} rows, got {len(df_with_target)})")
        print(f"   Consider reducing --lstm_sequence_length or collecting more data")
    
    # === –®–∞–≥ 5: –û–±—É—á–µ–Ω–∏–µ QuadEnsemble ===
    print(f"\n[Step 5] Training QuadEnsemble...")
    print(f"   This will train 4 models sequentially:")
    print(f"   1. RandomForest")
    print(f"   2. XGBoost")
    print(f"   3. LightGBM")
    print(f"   4. LSTM (this may take longer)")
    print()
    
    trainer = ModelTrainer()
    
    try:
        model, metrics = trainer.train_quad_ensemble(
            X=X,
            y=y,
            df=df_with_target,  # –ü–æ–ª–Ω—ã–π DataFrame –¥–ª—è LSTM
            rf_n_estimators=args.rf_n_estimators,
            rf_max_depth=args.rf_max_depth,
            xgb_n_estimators=args.xgb_n_estimators,
            xgb_max_depth=args.xgb_max_depth,
            xgb_learning_rate=args.xgb_learning_rate,
            lgb_n_estimators=args.lgb_n_estimators,
            lgb_max_depth=args.lgb_max_depth,
            lgb_learning_rate=args.lgb_learning_rate,
            lstm_sequence_length=args.lstm_sequence_length,
            lstm_hidden_size=args.lstm_hidden_size,
            lstm_num_layers=args.lstm_num_layers,
            lstm_epochs=args.lstm_epochs,
        )
        
        print(f"\nüìä QuadEnsemble Results:")
        print(f"  RandomForest CV Accuracy: {metrics['rf_metrics']['cv_mean']:.4f} (+/- {metrics['rf_metrics']['cv_std'] * 2:.4f})")
        print(f"  XGBoost CV Accuracy: {metrics['xgb_metrics']['cv_mean']:.4f} (+/- {metrics['xgb_metrics']['cv_std'] * 2:.4f})")
        print(f"  LightGBM CV Accuracy: {metrics['lgb_metrics']['cv_mean']:.4f} (+/- {metrics['lgb_metrics']['cv_std'] * 2:.4f})")
        print(f"  LSTM Accuracy: {metrics['lstm_metrics'].get('accuracy', 0):.4f}")
        print(f"\n  Ensemble Weights:")
        print(f"    RF:   {metrics['rf_weight']:.3f}")
        print(f"    XGB:  {metrics['xgb_weight']:.3f}")
        print(f"    LGB:  {metrics['lgb_weight']:.3f}")
        print(f"    LSTM: {metrics['lstm_weight']:.3f}")
        
    except Exception as e:
        print(f"‚ùå ERROR during training: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # === –®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    print(f"\n[Step 6] Saving model...")
    model_filename = f"quad_ensemble_{args.symbol}_{args.interval.replace('m', '')}.pkl"
    
    try:
        trainer.save_model(
            model,
            trainer.scaler,
            feature_engineer.get_feature_names(),
            metrics,
            model_filename,
            symbol=args.symbol,
            interval=args.interval.replace('m', ''),
            model_type="quad_ensemble",
        )
        
        print(f"‚úÖ Model saved: {model_filename}")
        print(f"\nüéâ Training completed successfully!")
        print(f"\nüí° Next steps:")
        print(f"   1. Test the model: python -m bot.ml.diagnose_model ml_models/{model_filename}")
        print(f"   2. Backtest: python backtest_ml_strategy.py --model ml_models/{model_filename} --symbol {args.symbol} --days 30")
        print(f"   3. Use in live trading: Enable ML strategy in config")
        print(f"   4. Compare with other models: Check backtest results")
        
    except Exception as e:
        print(f"‚ùå ERROR saving model: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
